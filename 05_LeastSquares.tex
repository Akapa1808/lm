\section{Least squares}

In this chapter we develop least squares for the general linear model $\mathbf{y=X\bfbet + \bfeps}.$

\subsection{Basics}

Let $\bX$ be a design matrix, notationally its
elements and column vectors are:
$$
\bX =
\left[
\begin{array}{ccc}
x_{11} & \ldots & x_{1p}  \\
\vdots & \ldots & \vdots \\
x_{n1} & \ldots & x_{np}
\end{array}
\right]
= [\bx_1 \ldots \bx_p].
$$
We are assuming that $n \geq p$ and $\bX$ is of full (column) rank. 
Consider ordinary least squares
\begin{equation}
\label{eq:ols}
f(\bbeta) = || \by - \bX \bbeta ||^2 = (\by - \bX \bbeta)' (\by - \bX \bbeta)
= \by' \by - 2  \bbeta' \bX' \by  + \bbeta' \bX' \bX \bbeta.
\end{equation}

To minimize $f(\bbeta)$, begin by
taking the derivative with respect to $\bbeta$:
$$
\frac{df}{d\bbeta}
= -2 \bX' \by + 2 \bX' \bX \bbeta.
$$
Solving for $0$ leads to the so called normal equations:
$$\bfX' \bfX \hat \bbeta = \bX' \by.$$
The matrix $\bX' \bX$ retains the same rank as $\bX$. Therefore,
it is a full rank $p\times p$ matrix and hence is invertible. We
can then solve the normal equations as:
\begin{equation}
\label{eq:betahat}
\hat \bbeta = (\bfX' \bfX)^{-1} \bfX' \bfy. \nonumber
\end{equation}
The Hessian of \eqref{eq:ols} is simply $2\bfX' \bfX$, which is positive
definite. (This is clear since for any non-zero vector, $\ba$, we have that
$\bX' \ba$ is non-zero since $\bX$ is full rank and then
$\ba' \bX' \bX \ba = ||\bX \ba||^2 > 0$.) Thus, the root of
our derivative is indeed a minimum. 

\subsection{Coding example}

\begin{verbatim}
> y = swiss$Fertility
> x = as.matrix(swiss[,-1])
> solve(t(x) %*% x, t(x) %*% y)
                       [,1]
Agriculture       0.1110005
Examination       0.4440591
Education        -0.7067362
Catholic          0.1170662
Infant.Mortality  2.9836617

> summary(lm(y ~ x - 1))$coef
                    Estimate Std. Error   t value     Pr(>|t|)
xAgriculture       0.1110005 0.07423536  1.495250 1.423257e-01
xExamination       0.4440591 0.31435258  1.412615 1.651367e-01
xEducation        -0.7067362 0.25008979 -2.825930 7.186594e-03
xCatholic          0.1170662 0.04859619  2.408958 2.046207e-02
xInfant.Mortality  2.9836617 0.31682721  9.417315 6.528210e-12
\end{verbatim}

%\subsection{A second derivation}
%
%If you know the answer first, it's possible to derive the 
%minimum to the least squares equation without taking any derivatives.
%\begin{eqnarray*}
%|| \by - \bX \bbeta ||^2 &=& || \by - \bX \hat \bbeta + \bX\hat \beta - \bX \bbeta || \\
%& = & ||\by - \bx \hat \bbeta||^2
%+ 2 (\by - \bx \hat \bbeta)^t (\bX \hat \bbeta - \bX \bbeta)
%+ ||\bX \hat \bbeta - \bX \bbeta||^2 \\
%& \geq & ||\by - \bx \hat \bbeta||^2
%+ 2 (\by - \bX \hat \bbeta)^t (\bX \hat \bbeta - \bX \bbeta) \\
%& = & ||\by - \bx \hat \bbeta||^2
%+ 2 (\by - \yhat)^t \bX (\hat \bbeta - \bbeta) \\
%& = & ||\by - \bX \hat \bbeta||^2
%+ 2 \by^t(\bI - \hatmat )^t \bX (\hat \bbeta - \bbeta) \\
%& = & ||\by - \bX \hat \bbeta||^2
%+ 2 \by^t(\bI - \hatmat ) \bX (\hat \bbeta - \bbeta) \\
%& = & ||\by - \bx \hat \bbeta||^2
%+ 2 \by^t(\bX - \hatmat \bX )(\hat \bbeta - \bbeta) \\
%& = & ||\by - \bx \hat \bbeta||^2
%+ 2 \by^t(\bX - \bX )(\hat \bbeta - \bbeta) \\
%& = & ||\by - \bx \hat \bbeta||^2
%\end{eqnarray*}
%Thus, any value of $\bbeta$ that we plug into the least squares equation
%is going to give us a larger norm than if we plug in $\hat \beta$ so that
%it is the unique minimum.
%Notice that going from line 5 to 6, we used the fact that $\bI - \hatmat$ is symmetric. (It is also idempotent.) Also, we used a fact that is very
%useful in general, $\bI - \hatmat$ is orthogonal to any linear combination
%of the columns of $\bX$. This is try since if $\bX \ba$ is such a combination,
%then
%$$
%\{\bI - \hatmat)\}bX \ba = \{\bX - \hatmat \bX\}\ba = (\bX - \bX) \ba = 0.
%$$
%This fact is extremely handy in working with linear models. 
%
%\subsection{Connection with linear regression}
%
%\label{sec:lslin}
%Recall that the slope from linear regression worked out to be
%$$
%\ip{\bx - \bar x \bone_n}{\bx - \bar x \bone_n}^{-1}\ip{\bx - \bar x\bone_n}{\by - \bar y \bone_n} 
%= \hat \sigma^{-2}_{X} \hat \sigma^2_{XY}
%$$
%where $\hat \sigma^2_{XY}$ is the empirical covariance between X and Y. 
%(We rewrote this formula using the more convenient correlation.) In this form it is the
%covariance between x and y divided by the variance of the x's. Let's consider
%extending this in our matrix results.
%
%Let $\bX = [\bone_n \bX_1]$, thus $\bX$ contains an intercept and then $p-1$ other
%regressors. Similarly let $\bbeta = (\beta_0, \ldots, \beta_{p-1})^t = (\beta_0 \beta_1^t)^t$. Consider now least squares
%$$
%||\by - \bX \bbeta||
%=||\by - \bone_n \beta_0 - \bX_1 \bbeta_1||
%$$
%If we were to hold $\beta_1$ fixed we are faced with a mean only regression
%problem and the solution to $\beta_0(\bbeta_1)$ is 
%$$
%\frac{1}{n}(\by - \bX_1 \bbeta_1)^t \bone_n
%= \bar y - \bar \bx^t \bbeta_1
%$$
%where $\bar \bx$ is the columnwise means of $\bX$. Plugging this back into
%our least squares equation for $\beta_0$ we get
%$$
%||\by - \bone_n \bar y - (\bX_1 - \bone_n \bar \bx^t )\bbeta_1||^2
%= || \tilde \by - \tilde \bX\bbeta_1||^2
%$$
%where $\tilde \by$ and $\tilde \bX$ are the centered versions of 
%$\by$ and $\bX$. This is again just the least squares equation with
%the centered variables and thus we get that
%$$
%\hat \beta_1 = (\tilde \bX^t \tilde \bX)^{-1} \tilde \bX \tilde \bY
%=
%\hat \beta_1 = \left(\frac{1}{n-1}\tilde \bX^t \tilde \bX\right)^{-1}  
%\frac{1}{n-1} \tilde \bX \tilde \bY.
%$$
%The matrix $\frac{1}{n-1}\tilde \bX^t \tilde \bX$ is the empirical 
%variance covariance matrix of the columns of $\bX$ while 
%$\frac{1}{n-1} \tilde \bX \tilde \bY$ is the vector of correlations
%of $\by$ with the columns of $\bX$. Therefore, if we include an intercept,
%our slope estimate is 
%$$
%\hat \bSigma_{XX}^{-1} \hat \brho_{XY}
%$$
%the inverse of the variance matrix associated with $\bX$ time the
%correlation matrix between $\bX$ and $\bY$. This draws an exact
%parallel with the result from linear regression.

\subsection{Projections}

The vector of fitted values is given by
$$
\hat \by = \hatm \by
$$
and the vector of residuals is given by
$$
\be = \by - \hat \by = (\bI - \hatm) \by.
$$
Thus, multiplication by the matrix $\bfH=\hatm$ takes any vector in
$\mathbb{R}^n$ and produces the fitted values. Note $\bfH$ is referred to as the `hat matrix' since it transforms $\bfy$ into $\hat \bfy$.
Similarly, multiplication by $(\bI - \hatm)$ produces the residuals. 
Notice that since the $\hat \by$ vector is a linear combination of the $\bX$,
it is orthogonal to the residuals, i.e.
$$
\hat \by' \be = \by' \hatm (\bI - \hatm)\by = 0.
$$

It is useful to think of least squares in the terms of projections.
Consider the column space of the design matrix, 
$\Gamma = \{ \bX \bbeta ~|~ \bbeta \in \mathbb{R}^p\}$. This $p$ 
dimensional subspace lies in $\mathbb{R}^n$.
Consider the vector $\by$ which lives in $\mathbb{R}^n$. 
%It can be shown that $\bfy$ can be uniquely decomposed as $ \bfy = \hat{\bfy} +
%{\bfe}$ where $\hat{\bfy}\in\\Gamma$ and  $\bfe \in \Gamma^\perp$, which is the orthogonal
%complement of $\Gamma$
Multiplication by the matrix $\hatm$ projects $\by$ into $\Gamma$. That
is, 
$$
\by \rightarrow \hatm \by
$$
is the linear projection map between $\mathbb{R}^n$ and $\Gamma$.
The point $\hat \by$ is the point in $\Gamma$ that is closest to $\by$ and
$\hat \bbeta$ is the specific linear combination of the columns
of $\bX$ that yields $\hat \by$. 
$\be$ is the vector connecting $\by$ and $\hat \by$, and it is
orthogonal to all elements in $\Gamma$.  

Thinking this helps us interpret statistical aspects of least squares.
First, if $\bW$ is any $p\times p $ invertible matrix, then the
fitted values, $\hat \by$ will be the same for the design matrix
$\bX \bW$. This is because the spaces
$$
\{\bX \bbeta ~|~ \bbeta \in \mathbb{R}^p\}
$$
and
$$
\{\bX \bW \bgamma ~|~ \bgamma \in \mathbb{R}^p\}
$$
are the same, since if $\ba = \bX \bbeta$ then $\ba = \bX \bgamma$ via the relationship $\gamma = \bW \bbeta$  and thus any element of the first
space is in the second. The same argument implies in the other direction,
thus the two spaces are the same.

Therefore, any linear reorganization of the columns of $\bX$ results in the
same column space and thus the same fitted values. Furthermore, any
addition of redundant columns to $\bX$ adds nothing to the column space,
and thus it's clear what the fit should be in the event that $\bX$ is
not full rank. Any full rank subset of the columns of $\bX$ defines
the same column and thus the same fitted values. 



\subsection{Another approach}

In this section we generate yet another derivation of least squares. 
For vectors $\ba$ (outcome) and $\bb$ (predictor), let us define the coefficient function as:
$$
c(\ba, \bb)
= \frac{\ip{\ba}{\bb}}{|| \bb ||^2}.
$$
and the residual function as
$$
e(\ba, \bb) = \ba - c(\ba, \bb) \bb
$$
We argue that the least squares estimate of outcome
$\by$ for predictor matrix $\bX = [\bx_1 \ldots \bx_p]$
can be obtained by taking successive residuals, in the following sense. Consider the least squares
equation holding $\beta_2, \ldots, \beta_p$ fixed, i.e.
\begin{equation}
\label{eq:itresid}
|| \by - \bx_1 \bbeta_1 - \ldots - \bx_p \bbeta_p||^2. 
\end{equation}
This is greater than or equal to if we replace $\beta_1$ by it's
estimate and keep with the remainder fixed. That estimate
being:
$$
c(\by - \bx_2 \bbeta_2 - \ldots, \bx_p \bbeta_p, \bx_1).
$$
Plugging that back into the least squares equation we
get
\begin{equation}
\label{eq:itresid2}
\eqref{eq:itresid}\geq ||e(\by, \bx_1) - 
e(\bx_2, \bx_1) \beta_2, \ldots, e(\bx_p, \bx_1) \beta_p||^2. \nonumber
\end{equation}
Thus we have a new least squares equation with 
all residuals having "removed" $\bx_1$ from all other
regressors and the outcome. Then we can repeat this
process again holding $\beta_3, \ldots, \beta_p$
fixed and obtain
$$
\eqref{eq:itresid2}\geq ||e\{e(\by, \bx_1), e(\bx_2, \bx_1)\} - 
e\{e(\bx_3, \bx_1), e(\bx_2, \bx_1)\} \beta_3, \ldots, e\{e(\bx_p, \bx_1), e(\bx_{2}, \bx_1)\} \beta_p||^2.
$$
This could then be iterated to the $p^{th}$ regressor.
Moreover, because we know the same inequalities will be
obtained no matter what order we get to the $p^{th}$
regressor we can conclude that the order of taking
residuals doesn't matter. Furthermore, picking the
$p^{th}$ coefficient was arbitrary as well, so the
same conclusion applies to all regressors: the
least squares estimate for all coefficients can be
obtained by iteratively taking residuals with all of the
other regressors (in any order). 

This is interesting for many reasons. First, it
is interesting to note that one need only 
regression through the origin to develop full
multivariable regression. Secondly it helps
us interpret our regression coefficients and
how they are "adjust" for the other variables.

There was nothing particular about using vectors.
Let $\bX = [\bX_1 \, \, \bX_2]$ be two submatrices of size
$p_1$ and $p_2$, and $\bbeta = (\bbeta_1' \, \,  \bbeta_2')'$ and consider minimizing:
$$
|| \by - \bX_1 \bbeta_1 - \bX_2 \bbeta_2||^2.
$$
If $\bbeta_2$ were held fixed, this would be maximized
at 
$$
\hat \bbeta_1(\bbeta_2) = 
(\bX_1' \bX_1)^{-1} \bX_1' (\by - \bX_2 \bbeta_2).
$$
Plugging that back in we obtain a smaller quantity
$$
||\{\bI - \bX_1(\bX_1' \bX_1)^{-1} \bX_1'\} \by
- \{\bI - \bX_1(\bX_1' \bX_1)^{-1} \bX_1' \}\bX_2 \bbeta_2
||^2
$$
This is equivalent to the residual of $\by$ having
regressed out $\bX_1$ and the residual matrix of
$\bX_2$ having regressed $\bX_1$ out of every column.
Thus out $\beta_2$ estimate will be the regression
matrix of these residuals. Again, this explains
why $\bbeta_2$'s estimate has been adjusted for
$\bX_1$, both the outcome and the $\bX_2$ predictors
have been orthogonalized to the space spanned
by the columns of $\bX_1$!


\subsection{Coding example}
Recall the Swiss fertility data.


\begin{verbatim}
> y = swiss$Fertility
> X = as.matrix(swiss[,-1])
> dim(X)
[1] 47  5
> X1 = X[,1:3]
> X2 = X[,4:5]

> ytilde = (I - X1%*%solve(t(X1)%*%X1)%*%t(X1))%*%y
> X2tilde = (I - X1%*%solve(t(X1)%*%X1)%*%t(X1))%*%X2
> beta2 = solve(t(X2tilde)%*%X2tilde)%*%t(X2tilde)%*%ytilde
> beta2
                      [,1]
Catholic         0.1170662
Infant.Mortality 2.9836617

> beta1 = solve(t(X1)%*%X1)%*%t(X1)%*%(y - X2%*%beta2)
> beta1
                  [,1]
Agriculture  0.1110005
Examination  0.4440591
Education   -0.7067362
\end{verbatim}



\subsection{Full row rank case}

In the case where $\bX$ is $n\times n$ of full rank,
then the columns of $\bX$ form a basis for $\mathbb{R}^n$.
In this case, $\hat \by = \by$, since $\by$ lives
in the space spanned by the columns of $\bX$. All
the linear model accomplishes is a lossless 
linear reorganization of $\by$. This is perhaps surprisingly
useful, especially when the columns of $\bX$ are
orthonormal ($\bX' \bX = \bI$). In this case, the
function that takes the outcome vector and converts
it to the coefficients is called a "transform". The
most well known versions of transforms are Fourier and
wavelet. 





